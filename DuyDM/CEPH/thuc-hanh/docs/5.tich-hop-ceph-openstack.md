# Ghi chép lại các bước tích hợp CEPH - Openstack

### Mục lục

[1. Mô hình triển khai](#mohinh)<br>
[2. IP Planning](#planning)<br>
[3. IP Các bước thực hiện tích hợp](#step)<br>

<a name="mohinh"></a>
## 1. Mô hình triển khai

![](../images/tich-hop-ceph-op/topocephop.png)

- Triển khai trên môi trường ảo hóa VMware ESXi.

- CEPH gồm 3 node CEPH mỗi node có 3 OSDs. Tham khảo cài đặt <a href="https://github.com/domanhduy/ghichep/blob/master/DuyDM/CEPH/thuc-hanh/docs/2.huong-dan-cai-dat-ceph-luminous.md" target="_blank">tại đây</a>!

- Openstack gồm 1 node Controller, 2 node Compute. Tham khảo cài đặt <a href="https://github.com/domanhduy/ghichep/blob/master/DuyDM/Openstack/install-openstack/docs/manual.md" target="_blank">tại đây</a>!

<a name="planning"></a>
## 2. IP Planning

![](../images/tich-hop-ceph-op/Screenshot_1701.png)


<a name="planning"></a>
## 3. Các bước thực hiện tích hợp

Về cơ bản Openstack có 3 service có thể tích hợp để lưu trữ xuống dưới CEPH.

- Glance (images): Lưu trữ các images xuống CEPH.

- Cinder (volume): Lưu trữ các ổ đĩa của máy ảo được tạo ra trên Openstack xuống dưới CEPH.

- Nova (conmpute): Mặc định khi VM được tạo sẽ sinh ra một file lưu thông tin file disk của VM đó trên chính node compute đó, có thể tích hợp xuống CEPH thông qua một `symlink`.

### 3.1. Chuẩn bị

**Cài đặt lib ceph python cho các node CTL, COM**

python-rbd: Thư viện hỗ trợ ceph.

ceph-common: Kết nối client vào ceph cluster.

```
yum install -y python-rbd ceph-common
```

**Tạo pool trên CEPH (Thực hiện trên node CEPH)**

Tùy thuộc và nhu cầu tích hợp để tạo những pool khác nhau với mục đích quản lý khác nhau. Sử dụng công cụ tính toán pool do CEPH hỗ trợ.

https://ceph.com/pgcalc/


![](../images/tich-hop-ceph-op/Screenshot_1702.png)

![](../images/tich-hop-ceph-op/Screenshot_1703.png)

```
ceph osd pool create backup 64
ceph osd pool set backup size 2
while [ $(ceph -s | grep creating -c) -gt 0 ]; do echo -n .;sleep 1; done

ceph osd pool create volumes 1024
ceph osd pool set volumes size 2
while [ $(ceph -s | grep creating -c) -gt 0 ]; do echo -n .;sleep 1; done

ceph osd pool create vms 64
ceph osd pool set vms size 2
while [ $(ceph -s | grep creating -c) -gt 0 ]; do echo -n .;sleep 1; done

ceph osd pool create images 128
ceph osd pool set images size 2
while [ $(ceph -s | grep creating -c) -gt 0 ]; do echo -n .;sleep 1; done
```

Mặc định cấu hình mặc định `mon_max_pg_per_osd = 200` chỉ cho tối đa 200 pg trên một OSD nên khi tạo số pg > 200 thì CEPH sẽ báo lỗi k khởi tạo được volume.

![](../images/tich-hop-ceph-op/Screenshot_1704.png)

Mặc định CEPH không cho xóa pool để tránh rủi ra mất mát data khi xóa nhầm pool. Để có thể xóa được pool phải chỉnh sửa file ceph config trong thư mục ceph-deploy và overwrite config tới các node khác thông qua ceph-deploy.

+ Overwrite config

```
ceph-deploy admin --overwrite-config ceph01 ceph02 ceph03
```

```
vi /ceph-deploy/ceph.conf

add

mon_allow pool delete = true
```

+ Giới hạn số lượng PG trên OSDs

```
vi /ceph-deploy/ceph.conf

add

mon max pg per osd  = 500
```

+ Restart service ceph MON

```
systemctl restart ceph-mon@ceph01
```

- Tạo thành công pool.

![](../images/tich-hop-ceph-op/Screenshot_1705.png)

